Hello everyone, welcome to this session. I'm Mohan from SimplyLearn and today we'll talk about interview questions for machine learning. Now this video will probably help you when you're attending interviews or machine learning positions and the attempt here is to probably consolidate 30 most commonly asked questions and to help you in answering these questions. We try our best to give you the best possible answers but of course what is more important here is rather than the theoretical knowledge you need to kind of add to the answers or supplement your answers with your own experience. So the responses that we put here are a bit more generic in nature so that there are some concepts that you are not clear. This video will help you in kind of getting those concepts cleared up as well but what is more important is that you need to supplement these responses with your own practical experience. Okay so with that let's get started. So one of the first questions that you may face is what are the different types of machine learning. Now what is the best way to respond to this? There are three types of machine learning. If you need any material you will always be told there are three types of machine learning but what is important is you would probably very better of emphasizing that there are actually two main types of machine learning which is supervised and unsupervised and then there is a third type which is reinforcement learning. So supervised learning is where you have some historical data and then you feed that data to your model to learn. Now you need to be aware of a keyword that they will be looking for which is labeled data right. So if you just say past data or historical data the impact may not be so much you need to emphasize on labeled data. So what is labeled data? Basically let's say if you are trying to do a training model for classification you need to be aware of for your existing data which class each of the observations belong to right. So that is what is labeling. So it is nothing but a fancy name you must learn any of well but just make it a point to throw in that keyword labeled so that will have the writing factor. Okay so that is what is supervised learning when you have existing labeled data which you then use to train your model that is known as supervised learning and unsupervised learning is when you don't have this labeled data. So you have data it is not labeled. So the system has to figure out a way to do some analysis on this. So that is unsupervised learning and you can then add a few things like what are the ways of performing supervised learning and unsupervised learning or what are some of the techniques. So supervised learning we perform or we do regression and classification and unsupervised learning we do clustering and clustering can be of different types and then regression can be of different types but you don't have to probably elaborate so much if they are asking for just the different types you can just mention these and just at a very high level it is but if they want you to elaborate give examples then of course if there is a different question for that we will see that later. Then the third so we have supervised then we have unsupervised and then reinforcement you need to provide a little bit of information around that as well because it is sometimes a little difficult to come up with a good definition for reinforcement learning. So you may have to a little bit elaborate on how reinforcement learning works right. So reinforcement learning works in such a way that it basically has two parts to it one is the agent and the environment and the agent basically is working inside of this environment and it is given a target that it has to achieve and every time it is moving in the direction of the target so the agent basically has to take some action and every time it takes an action which is moving the agent towards the target towards a goal a target is nothing but a goal then it is rewarded and every time it is going in a direction where it is away from the goal then it is punished so that is the way you can explain and this is used primarily or it is very very impactful or teaching the system to learn games and so on examples of this are basically used in AlphaGo you can throw that as an example where AlphaGo used reinforcement learning to actually learn to play the game of Go and finally it defeated Go World Champion right. There is much of information that would be good enough okay then there could be a question on overfitting so the question could be what is overfitting and how can you avoid it so what is overfitting let's first try to understand the concept because sometimes overfitting maybe little difficult to understand overfitting is a situation where the model has kind of memorized the data so this is an equivalent of memorizing the data so we can draw an analogy so that it becomes easy to explain this now let's say you're teaching a child about some recognizing some fruit so something like that okay and you're teaching this child about recognizing let's say three fruits apples oranges and pineapples okay so this is a small child and for the first time you're teaching the child to recognize fruits then so what will happen so this is very much like that is your training data set so what you will do is you'll take a basket of fruits which consists of apples oranges and pineapples okay and you take this basket to this child and there may be let's say hundreds of these fruits so you take this basket to this child and keep showing each of this root and then first time obviously the child will not know what it is so you show an apple and you say hey this is apple then you show maybe an orange and say this is orange and so on and so and then again you keep repeating that right so till that basket is over this is basically how training work in machine learning also that's how training works so till the basket is completed maybe hundred fruits you keep showing this child then the process what has happened the child has pretty much memorized these so even before you finish that basket right by the time you are halfway through the child has learned about recognizing the apple or in-gen pineapp now what will happen after halfway through initially you remember it made mistakes in recognizing but halfway through now it has learned so every time you show a fruit it will exactly 100% accurately it will identify it will say the child will say this is an apple this is an orange and if you show a pineapple it will say this is a pineapple right so that means it has kind of memorized this data now let's say you bring another basket of fruits and it will have a mix of maybe apples which were already there in the previous set but it will also have an addition to apple it will probably have a banana or maybe another fruit like a jackfruit right so this is an equivalent of your test dataset which the child has not seen before some odds of it it probably has seen like the apples it has seen but this banana and jackfruit it has not seen so then what will happen in the first round which is an equivalent of your training dataset towards the end it has 100% it was telling you what the fruits are right apple was accurately recognized orange ore was accurately recognized and pineapples were accurately recognized right so that is like a hundred percent accuracy but now when you get another a fresh set which were not all of the original one what will happen all the apples maybe it will be able to recognize correctly but all the others like the jackfruit or the banana will not be recognized by the child right so this is an analogy this is an equivalent of overfitting so what has happened during the training process it is able to recognize or reach 100% accuracy maybe very high accuracy okay and we call that as very low loss right so that is the technical term so the loss is pretty much zero and accuracy is pretty much 100% whereas when you use testing there will be a huge error which means the loss will be pretty high and therefore the accuracy will be also low okay this is known as overfitting this is basically a process where training is done training process is it goes very well almost reaching 100% accuracy but while testing it really drops now how can you avoid it so that is the extension of this question there are multiple ways of avoiding overfitting there are techniques like what we call regularization that is the most common technique that is used for avoiding overfitting and within regularization there can be a few other subtypes like dropout in case of neural networks and a few other examples but I think if you give example or if you regularization as the technique probably that should be sufficient so so there will be some questions where the interviewer will try to test your fundamentals and your knowledge and depth of knowledge and so on and so forth and then there will be some questions which are more like quick questions that will be more to stump you okay then the next question is around the methodology so when we are performing machine learning training we split the data into training and test right so this question is around that so the question is what is training set at this set in machine learning model and how is the split done so the question can be like that so in machine learning when we are trying to train the model so we have a three step process we train the model and then we test the model and then once we are satisfied with the test only then we deploy the model so what happens in the train and test is that you remember the labeled data so let's say you have a thousand records with labeling information now one way of doing it is you use all the thousand records for training and then maybe right which means that you have exposed all this thousand records during the training process and then you take a small set of the same data and then you say okay I will test it with this okay and then you probably what will happen you may get some good results all right but there is a flaw there what is the flaw this is very similar to human beings it is like you are showing this model the entire data as a part of training okay so obviously it has become familiar with the entire data so when you are taking a part of that again and you are saying that I want to test it obviously you will get good results so that is not a very accurate way of testing so that is the reason what we do is we have the label data of this thousand records or whatever we set aside before starting the training process we set aside a portion of that data and we call that test set and the remaining we call as training set and we use only this for training our model now the training process remember is not just about asking one round of this data set so let's say now your training set has 800 records it is not just one time you pass this 800 records what you normally do is you actually as a part of the training you may ask this data through the model multiple times so this thousand records may go through the model maybe 10, 15, 20 times till the training is perfect till the accuracy is high till the errors are minimized okay now so which is fine which means that you are that is what is known as the model has seen your data and gets familiar with your data and now when you bring your test data what will happen is this is like some new data because that is where the real test is now you're trained the model and now you are testing the model with some data which is kind of new that is like a situation like a realistic situation because when the model is deployed that is what will happen it will receive some new data not data that it has already seen right so this is a realistic test so you put some new data so this data which you have set aside is for the model it is new and if it is able to accurately predict the values that means your training has worked okay the model got trained probably but let's say while you're testing this with this test data you're getting lot of errors that means you need to probably either change your model or retrain with more data and things like that now coming back to the question of how do you split this what should be the ratio there is no fixed number again this is like individual preferences some people split it into 50 50 50 percent test and 50 percent training some people prefer to have a larger amount for training and a smaller amount for test so they can go by either 60 40 or 70 30 or some people even go with some odd numbers like 65 35 or 63.33 and 33 which is like one third and two third so there is no fixed rule that it has to be something that is the ratio has to be this you can go by your individual preferences all right then you may have questions around data handling data manipulation or what do you call data management preparation so these are all some questions around that area there is again no one answer one single good answer to this it really varies from situation to situation and depending on what exactly is the problem what kind of data it is how critical it is what kind of data is missing and what is the type of corruption so there are a whole lot of things which is a very generic question and therefore you need to be a little careful about responding to this as well so probably have to illustrate this again if you have experience in doing this kind of work in handling data you can illustrate with examples saying that I was on one project where I received this kind of data these were the columns where data was not filled or these were the this many rows where the data was missing that would be in fact a perfect way to respond to this question but if you don't have that obviously you have to provide some good answer I think it really depends on what exactly the situation is and there are multiple ways of handling the missing data or correct data now let's take a few examples now let's say you have data where some values in some of the columns are missing and you have pretty much half of your data having these missing values in terms of number of rows okay that could be one situation another situation could be that you have records are data missing but when you do some initial calculation how many records are corrupt or how many rows or observations as we call it as the missing data let's assume it is very minimal like 10% okay now between these two cases how do you so let's assume that this is not a mission critical situation and in order to fix this 10% of the data the effort that is required is much higher and obviously effort means also time and money right so it is not so mission critical and it is okay to let's say get rid of these records so obviously one of the easiest ways of handling the data part or missing data is remove those records or remove those observations from your analysis so that is the easiest way to do but then the downside is as I said in as in the first case if let's say 50% of your data is like that because some columnar the other way is missing so it is not like every in every place in every row the same column is missing but you have in maybe 10% of the records column one is missing another 10% column two is missing another 10% column three is missing and so on and so far so it adds up to maybe half of your data set so you cannot completely remove half of your data set then the whole purpose is lost okay so then how do you handle then you need to come up with ways of filling up this data with some meaningful value right that is one way of handling so when we say meaningful value what is that meaningful let's say for a particular column you might want to take a mean value for that column and fill wherever the data is missing fill up with that mean value so that when you're doing the calculations your analysis is not completely way off so you have values which are not missing first of all so your system will work number two these values are not so completely out of whack that your whole analysis goes for a task right there may be situations where if the missing values instead of putting mean maybe a good idea to fill it up with the minimum value or with a zero so or with the maximum value again as I said there are so many possibilities so there is no like one correct answer for this you need to basically talk around this and illustrate with your experience as I said that would be the best otherwise this is how you need to handle this question okay so then the next question can be how can you choose a classifier based on a training set data size so again this is one of those questions where you probably do not have like one size fits all on so first of all you may not let's say decide your classifier based on the training set size maybe not the best way to decide the type of the classifier and even if you have to there are probably some thumb rules which we can use but then again every time so in my opinion the best way to respond to this question is you need to try out few classifiers irrespective of the size of the data and you need to then decide on your particular situation which of these classifiers are the right ones this is a very generic issue so you will never be able to just by if somebody defines a problem to you and somebody when if they show the data to you or tell you what is the data or even the size of the data I don't think there is a way to really say that yes this is the classifier that will work here no that's not the right way so you need to still you know test it out get the data try out a couple of classifiers and then only you will be in a position to decide which classifier to use you try out multiple classifiers see which one gives the best accuracy and only then you can decide then you can have a question around confusion matrix so the question can be explained confusion matrix right so confusion matrix I think the best way to explain it is by taking an example and drawing like a small diagram otherwise it can really become tricky so my suggestion is to take a piece of pen and paper and explain it by drawing a small matrix and confusion matrix is about to find out this is you especially in classification learning process when you get the results and the model predicts the results you compare it with the actual value and try to find out what is the accuracy okay so in this case let's say this is an example of a confusion matrix and it is a binary matrix so you have the actual values which is the labeled data right and which is so you have how many yes and how many no so you have that and you have the predicted values how many yes and how many no right so the total actual values the total yes is 12 plus 130 and they are shown here and the actual value knows are 9 plus 3 12 okay so that is what this information here is so this is about the actual and this is about the predicted similarly the predicted values there are yes are 12 plus 3 15 yeses and no are 1 plus 9 10 no's okay so this is the way to look at this confusion matrix okay and out of this what is the meaning conveyed so there are two or three things that needs to be explained outright the first thing is for a model to be accurate the values across the diagonal should be high like in this case right that is one number two the total sum of these values is equal to the total observations in the test data set so in this case for example you have 12 plus 3 15 plus 10 25 so that means we have 25 observations in our test data set okay so these are the two things you need to first explain that the total sum in this matrix the numbers is equal to the size of the test data set and the diagonal values indicate the accuracy so by just by looking at it you can probably have an idea about is this an accurate model is the model being accurate if they're all spread out equally in all these four boxes that means probably the accuracy is not very good okay now how do you calculate the accuracy itself right how do you calculate the accuracy itself so it is a very simple mathematical calculation you take some of the diagonals right so in this case it is 9 plus 12 21 and divided by the total so in this case what will it be let's me take a pen so your your diagonal values is equal if I say t is equal to 12 plus 9 so that is 21 right and the total data set is equal to right we just calculated it is 25 so what is your accuracy it is 21 by your accuracy is equal to 21 by 25 and this turns out to be about 85% right so this is 85% so that is our accuracy okay so this is the way you need to explain draw diagram give an example maybe maybe a good idea to be prepared with an example so that it becomes easy for you don't have to calculate this numbers on the fly right so a couple of hints are that you take some numbers which are with which add up to 100 that is always a good idea so you don't have to really do this complex calculations so the total value will be 100 and then diagonal values it divide once you find a diagonal values that is equal to your percentage okay all right so the next question can be a related question about false positive and false negative so what is false positive and what is false negative now once again the best way to explain this is using a piece of paper and pen otherwise it will be pretty difficult to explain so we use the same example of the confusion matrix and we can explain that so a confusion matrix looks somewhat like this and when we just look somewhat like this and we continue with the previous example where this is the actual value this is the predicted value and in the actual value we have 12 plus 113 yeses and 3 plus 912 noes and the predicted values there are 12 plus the 15 yeses and 1 plus 910 noes okay now this particular case which is the false positive what is a false positive first of all the second word which is positive okay is referring to the predicted value so that means the system has predicted it as a positive but the real value so this is what the false comes from but the real value is not positive okay that is the way you should understand this term false positive or even false negative so false positive so positive is what your system has predicted so where is that system predicted this is the one positive is what yes so you basically consider this row okay now if you consider this row so this is this is all positive values this entire row is positive values okay now the false positive is the one where the value actual value is negative predicted value is positive but the actual value is negative so this is a false positive right and here is a true positive so the predicted value is positive and the actual value is also positive okay I hope this is making sense now let's take a look at what is false negative false negative. So negative is the second term. That means that is the predicted value that we need to look for. So which are the predicted negative values? This row corresponds to predicted negative values. All right. So this row corresponds to predicted negative values. And what they are asking for falls. So this is the row for predicted negative values. And the actual value is this one. Right. This is predicted negative. And the actual value is also negative. Therefore, this is a true negative. So the false negative is this one. Predicted is negative, but actually is positive. Right. So this is the false negative. So this is the way to explain. And this is the way to look at false positive and false negative. Same way, there can be true positive and true negative as well. So again, positive, the second term you will need to use to identify the predicted row. Right. So if we say true positive positive, we need to take for the predicted part. So predicted positive is here. Okay. And then the first term is for the actual. So true positive. So true in case of actual is yes. Right. So true positive is this one. Okay. And then in case of actual, the negative, now we are talking about let's say true negative, true negative. Negative is this one. And the true comes from here. So this is true negative. Right. Nine is true negative. The actual value is also negative. And the predicted value is also negative. Okay. So that is the way you need to explain this. The terms false positive false negative and true positive true negative. Then you might have a question like what are the steps involved in the machine learning process or what are the three steps in the process of developing a machine learning model. Right. So it is around the methodology that is applied. So basically the way you can probably answer in your words, but the way the model development of the machine learning model happens is like this. So first of all, you try to understand the problem and try to figure out whether it is a classification problem or a regression problem based on that you select a few algorithms. And then you start the process of training these models. Okay. So you can either do that or you can after you the legends, you can probably decide that there is one particular algorithm which is most suitable. Usually it happens to trial and error process, but at some point you will decide that, okay, this is the model we are going to use. Okay. So in that case, we have the model algorithm and the model decided and then you need to do the process of training the model and testing the model. And this is where if it is supervised learning, you split your data, the label data into training dataset and test dataset. And you use the training dataset to train your model and then you use the test dataset to check the accuracy, whether it is working fine or not. So you test the model before you actually put it into production. Right. So once you test the model, you satisfy it's working fine. Then you go to the next level which is putting it for production and then in production obviously new data will come and the inference happens. So the model is readily available and only thing that happens is new data comes and the model predicts the values, whether it is regression or classification. Now, so this can be an iterative process. So it is not a straightforward process where you do the training for the testing and then you move it to production. Now, so during the training and test process, there may be a situation where because of either overfitting or things like that, the test doesn't go through which means that you need to put that back into the training process. So that can be an iterative process. Not only that, even if the training and test goes to properly and you deploy the model in production, there can be a situation that the data that actually comes, the real data that comes with that, this model is failing. So in which case you may have to once again go back to the drawing mode or initially it will be working fine, but over a period of time, maybe due to the change in the nature of the data, once again the accuracy will iterate. So that is again a recursive process. So once in a while you need to keep checking whether the model is working fine or not and if required, you need to tweak it and modify it and so on and so forth. So let me add this is a continuous process of tweaking the model and testing it and making sure it is up to date. Then you might have question around deep learning. So because deep learning is now associated with AI, artificial intelligence and so on, so can be as simple as what is a deep learning. So I think the best way to respond to this could be deep learning is a part of machine learning and then obviously the question would be then what is the difference, right? So deep learning you need to mention there are two key parts that interviewer will be looking for when you are defining deep learning. So first is of course deep learning is a subset of machine learning. So machine learning is still the bigger let's say scope and deep learning is one part of it. So then what exactly is the difference? Deep learning is primarily when we are implementing these our algorithm or when we are using neural networks for doing our training and classification and regression and all that, right? So when we use neural network then it is considered as a deep learning and the term deep comes from the fact that you can have several layers of neural networks and these are called deep neural networks and therefore the term deep learning. The other difference between machine learning and deep learning which the interviewer may be wanting to hear is that in case of machine learning the feature engineering is done manually. What do we mean by feature engineering? Basically when we are trying to train our model we have our training data, right? So we have our training label data and this data has several let's say if it is a regular table it has several columns. Now each of these columns actually has information about a feature, right? So if we are trying to predict the height, weight and so on and so forth. So these are all features of human beings. Let's say we have census data and we have all these. So those are the features. Now there may probably 50 or 100 in some cases there may be hundreds such features. Now all of them do not contribute to our model, right? So we as a data scientist we have to decide whether we should take all of them, all the features or we should throw away some of them because again if we take all of them number one of course your accuracy will probably get affected but also there is a computational part. So if you have so many features and then we have so much data it becomes very tricky. So in case of machine learning we manually take care of identifying the features that do not contribute to the learning process and thereby we eliminate those features and so on, right? So this is known as feature engineering and in machine learning we do that manually whereas in deep learning where we use neural networks the model will automatically determine which features to use and which to not use and therefore feature engineering is also done automatically. So this is a explanation these are two key things probably will add value to your response. All right so the next question is what is the difference between or what are the differences between machine learning and deep learning? So here this is a quick comparison table between machine learning and deep learning and in machine learning learning enables machines to take decisions on their own based on past data. So here we are talking primarily of supervised learning and it needs only a small amount of data for training and then works well on low end systems so you don't need large machines and most features need to be identified in advance and manually coded. So basically the feature engineering part is done manually and the problem is divided into parts and solved individually and then combined. So that is about the machine learning part. In deep learning deep learning basically enables machines to take decisions with the help of artificial neural networks. So here in deep learning we use neural networks. So that is the key differentiator between machine learning and deep learning and usually deep learning involves a large amount of data and therefore the training also requires usually the training process requires high end machines because it needs a lot of computing power and the machine learning features are the feature engineering is done automatically. So the neural networks takes care of doing the feature engineering as well and in case of deep learning therefore it is said that the problem is handled and to end. So this is a quick comparison between machine learning and deep learning in case you have that kind of a question. Then you might get a question around the uses of machine learning or some real life applications of machine learning in modern business. The question may be worded in different ways but the meaning is how exactly is machine learning used or actually supervised machine learning. It could be a very specific question around supervised machine learning. So this is like give examples of supervised machine learning use of supervised machine learning in modern business. So that could be the next question. So there are quite a few examples or quite a few use cases if you will for supervised machine learning. The very common one is email spam detection. So you want to train your application or your system to detect between spam and on spam. So this is a very common business application of supervised machine learning. So how does this work the way it works is that you obviously have historical data of your emails and they are categorized as spam and not spam. So that is what is the labeled information. And then you feed this information or all these emails as an input to your model. And the model will then get trained to detect which of the emails are to detect which is spam and which is not spam. So that is the training process. And this is supervised machine learning because you have labeled data. You already have emails which are tagged as spam or not spam and then you use that to train your model. So this is one example. Now there are a few industry specific applications for supervised machine learning. One of the very common ones is a healthcare diagnostics. In healthcare diagnostics you have these images and you want to train models to detect whether from a particular image whether it can find out if the person is sick or not, whether a person has cancer or not. So this is a very good example of supervised machine learning. Here the way it works is that existing images. It could be x-ray images, it be MRI or any of these images are available and they are tag saying that look at this x-ray image is defective or the person has an illness or it could be cancer which is very illness. So it is tagged as defective or clear or good image and defective. Something like that. So we come up with a binary or it could be multi-class as well saying that this is defective to 10 person, this is 25 person and so on. But let's keep it simple. You can give an example of just a binary classification that would be good enough. So you can say that in healthcare diagnostics using image we need to detect whether a person is ill or whether a person is having cancer or not. So here the way it works is you feed labeled images and you allow the model to learn from that. So that when new images fed it will be able to predict whether this person is having that illness or not having cancer or not. So I think this would be a very good example for supervised machine learning in modern business. Then we can have a question like so we've been talking about supervised and unsupervised and so there can be a question around semi-supervised machine learning. What is semi-supervised machine learning? Now semi-supervised learning as the name suggests it falls between supervised learning and unsupervised learning but for all practical purposes it is considered as a part of supervised learning. And the reason this has come into existence is that in supervised learning you need labeled data. So all your data for training your model has to be labeled. Now this is a big problem in many industries or in many under many situations. Getting the label data is not that easy because there's a lot of effort in labeling this data. Let's take an example of agnostic images. We can just let's say take x-ray images. Now there are actually millions of x-ray images available all over the world but the problem is they are not labeled. So their images are there but whether it is effective or whether it is good that information is not available along with it right in a form that it can be used by a machine which means that somebody has to take a look at these images and usually it should be like a doctor and then say that okay yes this image is clean and this image is cancerous and so on and so on. Now that is a huge effort by itself. So this is where semi-supervised learning comes into play. So what happens is there is a large amount of data maybe a part of it is labeled then we try some techniques to label the remaining part of the data so that we get completely labeled data and then we train our model. So I know there's a little long-winding explanation but unfortunately there is no quick and easy definition for semi-supervised machine learning. This is the only way probably to explain this concept. We may have another question as what are unsupervised machine learning techniques or what are some of the techniques used for performing unsupervised machine learning so it can be worded in in different ways. So how do we answer this question? So unsupervised learning you can say that there are two types clustering and association and clustering is a technique where similar objects are put together and there are different ways of finding similar objects so their characteristics can be measured and if they have in most of the characteristics if they are similar then they can be put together. This is clustering. Then association you can I think the best way to explain association is with an example. In case of association you try to find out how the items are linked to each other. So for example if somebody bought maybe a laptop the person has also purchased mouse so this is more in a e-commerce scenario for example so you can give this as an example. So people who are buying laptops are also buying mouse so that means there is an association between laptops and mouse or maybe people who are buying bread are also buying butter so that is association that can be created so this is unsupervised learning one of the techniques. Okay. Alright then we have very fundamental question what is the difference between supervised and unsupervised machine learning. So machine learning these are the two main types of machine learning supervised and unsupervised and in case of supervised and again here probably the key word that the person may be wanting to hear is labeled data. Now very often people say we have historical data and if we run it it is supervised and if we don't have historical data yes but you may have historical data but if it is not labeled then you cannot use it for supervised learning so it is very key to understand that we put in that keyword labeled okay. So when we have labeled data for training our model then we can use supervised learning and if we do not have labeled data then we use unsupervised learning and there are different algorithms available to perform both of these types of trainings. So there can be other questions a little bit more theoretical and conceptual in nature this is about inductive machine learning and inductive machine learning. So the question can be what is the difference between inductive machine learning and deductive machine learning or somewhat in that manner so that the exact phrase or exact question can vary I think can ask for examples and things like that but that could be the question. So let's first understand what is inductive and deductive training. Inductive training is induced by somebody and you can illustrate that with a small example I think that always helps. So whenever you're doing some explanation try as much as possible as I said to give examples from your work experience or give some analogies and that will also help a lot in explaining as well and for the interviewer also to understand. So here we'll take an example or rather we will use an analogy so inductive training is when we use some knowledge or the learning process into a person without the person actually experiencing it okay what can be an example so we can probably tell the person or show a person a video that fire can burn the thing burn his finger or fire can cause damage. So what is happening here this person has never probably seen fire or never seen anything getting damaged by fire but just because he has seen this video he knows that fire is dangerous and if fire can cause damage so this is inductive learning. Compared to that what is deductive learning so here you draw conclusion or the person draws conclusion out of experience so we will stick to the analogy so compared to the showing a video let's assume a person is allowed to play with fire and then he figures out that if he puts his finger it's burning or if it throws something into the fire it burns so he is learning through experience so this is known as deductive learning okay so you can have applications or models that can be trained using inductive learning or deductive learning all right I think probably that explanation will be sufficient. The next question is are KNN and K means clustering similar to one another or are they same right because the letter K is kind of common between them okay so let us take a little while to understand what these two are one is KN and another is KNNs a KNN stands for K nearest neighbors and K means of course is the clustering mechanism now these two are completely different except for the letter K being common between them KNN is completely different K means clustering is completely different KNN is a classification process and therefore it comes under supervised learning whereas K means clustering is actually unsupervised okay when you have KNN when you want to implement KNN which is basically K nearest neighbors the value of K is a number so you can say K is equal to 3 you want to implement KNN with K is equal to 3 so which means that it performs the classification in such a way that how does it perform the classification so it will take three nearest objects and that's why it's called nearest neighbor so basically based on the distance it will try to find out its nearest objects that are it's a three of the nearest objects and then it will check whether the class they belong to which class right so if all three belong to one particular class obviously this new object is also classified as that particular class but it is possible that they may be from two or three different classes okay so let's say they are from two classes and then they are from two classes now usually you take a odd number you assign a odd number to so if there are three of them and two of them belong to one class and then one belongs to another class so this new object is assigned to the class to which the two of them belong now the value of K is sometimes tricky whether should you use three which would you use five should you use seven that can be tricky because the ultimate classification can also vary so it's possible that if you're taking K as three the object is probably in one particular class but if you take a K is equal to five maybe the object will belong to a different class because when you're taking three of them probably two of them belong to a class one and one belong to class two whereas when you take five of them it is possible that only two of them belong to class one and the three of them belong to class two so which means that this object will belong to class two right so you see that so it is the class allocation can vary depending on the value of K now K means on the other hand is a clustering process and it is unsupervised what it does is the system will basically identify how the objects are how close the objects are with respect to some of their features okay and but the similarity of course is the letter K and in case of K means also we specify its value and it could be three or five or seven there is no technical limit as such but it can be any number of clusters that you can create okay so based on the value that you provide the system will create that many clusters of similar objects so there is a similarity to that extent that K is a number in both the cases but actually these two are completely different processes we have what is known as naive base classifier and people often get confused thinking that naive base is the name of the person who found this classifier order who developed this classifier which is not 100% true base is the name of the person Bavis is the name of the person but naive is not the name of the person right so naive is basically an English word and that has been added here because of the nature of this particular classifier naive base classifier is a probability based classifier and it makes some assumptions that presence of one feature of a class is not related to the presence of any other feature of maybe other classes right so which is not a very strong or not a very what do you say accurate assumption because these features can be related and so on but even if we go with this assumption this whole algorithm works very well even with this assumption and that is the good side of it but the term comes from there so that is the explanation that you can then there can be question around reinforcement learning it can be paraphrased in multiple ways one could be can you explain how a system can play a game of chess using reinforcement learning or it can be any game so the best way to explain this is again to talk a little bit about what reinforcement learning is about and then elaborate on that to explain the process so first of all reinforcement learning has an environment and an agent and the agent is basically performing some actions in order to achieve a certain goal and this goals can be anything either it is related to game then the goal could be that you have to score very high score high value high number or it could be that your number of lives should be as high as possible don't lose lives so these could be some of them a more advanced examples could be for driving the automotive industry self-driving cars they actually also make use of reinforcement learning to teach the car how to navigate through the roads and so on and so forth that is also another example now how does it work so if the system is basically there is an agent in an environment and every time the agent takes a step or performs a task which is taking it towards the goal the final goal let's say to maximize this score or to minimize the number of lives and so on or minimize the debts for example it is rewarded and every time it takes a step which goes against that goal and contrary or reverse direction it is penalized okay so it is like a carrot and stick system now how do you use this to create a game of chess or to create a system to play a game of chess now the way this works is and this could probably go back to this alpha go example where alpha go defeated a human champion so the way it works is reinforcement learning the system is allowed for example in this case we are talking about chess so we allow the system to first of all watch playing a game of chess so it could be with a human being or it could be the system itself there computer games of chess right so either this new learning system has to watch that game or watch a human being play the game because this is reinforcement learning is pretty much all visual so when you're teaching the system to play a game the system will not actually go behind this instance to understand the logic of your software of this game or anything like that it is just visually watching the screen and then it learns okay so reinforcement learning to a large extent it works on that so you need to create a mechanism whereby your model will be able to watch somebody playing the game and then you allow the system also to start playing the game so it pretty much starts from scratch okay and as it moves forward it it's like right at the beginning the system really knows nothing about the game of chess okay so initially it is a clean slate it just starts by observing how you're playing so it will make some random moves and keep losing badly but then what happens is over a period of time so you need to now allow the system or you need to play with the system not just one two three four or five times but hundreds of times thousands of times maybe even hundreds of thousands of times and that's exactly how AlphaGo has done it played millions of games between itself and the system right so for the game of chess also you need to do something like that you need to allow the system to play chess and and learn on its own over a period of repetition so I think you can probably explain it to this much to this extent and it should be sufficient now this is another question which is again somewhat similar but here the size is not coming into picture so the question is how will you know which machine learning algorithm to choose for your classification problem now this is not only classification problem it could be a regression problem I would like to generalize this question so if somebody else here how will you choose how will you know which algorithm to use the simple answer is there is no way you can decide exactly saying that this is the algorithm I am going to use in a variety of situations there are some guidelines like for example you will obviously depending on the problem you can say whether it is a classification problem or a regression problem and then in that sense you are kind of restricting yourself to if it is a classification problem there are you can only apply a classification algorithm right to that extent you can probably let's say limit the number of algorithms but now within the classification algorithms you have decision trees you have SVM you have logistic regression is it possible to outright say yes sirth for this particular problem since you have explained this now this is the exact algorithm that you can use that is not possible okay so we have to try out a bunch of algorithms see which one gives us the best performance and best accuracy and then decide to go with that particular algorithm so in machine learning a lot of it happens through trial and error there is no real possibility that anybody can just by looking at the problem or understanding the problem tell you that okay in this particular situation this is exactly the algorithm that you should use then the questions may be around application of machine learning and this question is specifically around how Amazon is able to recommend other things to buy so this is around recommendation engine how does it work how does the recommendation engine work so this is basically the question is all about so the recommendation engine again works based on various inputs that are provided obviously something like you know Amazon website or e-commerce site like Amazon collects a lot of data around the customer behavior who is purchasing what and if somebody is buying a particular thing they're also buying something else so this kind of association right so this is not supervised learning we talked about they use this to associate and link or relate items and that is one part of it so they kind of build association between items saying that somebody buying this is also buying this that is one part of it then they also profile the users right based on their age their gender their geographic location they will do some profiling and then when somebody is logging in and when somebody is shopping kind of the mapping of these two things are done they try to identify obviously if you have logged in then they know who you are and your information is available like for example your age maybe your gender and where you're located what you purchased earlier right so all this is taken and the recommendation engine basically uses all this information and comes up with recommendations for a particular user so that is how the recommendation engine work all right then the question can be something very basic like when will you go for classification versus regression right when do you do classification instead of regression or when will you use classification instead of regression now yes so so this is basically going back to the understanding of the basics of classification and regression so classification is used when you have to identify or categorize things into discrete classes so the best way to respond to this question is to take up some examples and use it otherwise it can become a little tricky the question may sound very simple but explaining it can sometimes be very tricky in case of regression the use of course there will be some keywords that they will be looking for so I'll just you need to make sure you use those keywords one is the discrete values another is the continuous values so for regression if you are trying to find some continuous values you use regression whereas if you're trying to find some discrete values you use classification and then you need to illustrate what are some of the examples so classification is like let's say there are images and you need to put them into a classes like cat, dog, elephant, tiger, something like that so that is a classification problem or it can be that is a multi-class classification problem it could be binary classification problem like for example whether a customer will buy or he will not buy that is a classification binary classification it can be in the weather forecast area now whether forecast is again combination of regression and classification because on the one hand you want to predict whether it's going to rain or not that's a classification problem that's a binary classification right whether it's going to rain or not rain however you also have to predict what is going to be the temperature tomorrow right now temperature is a continuous value you can't answer the temperature in a yes or no kind of a response right so what will be the temperature tomorrow so you need to give a number which can be like 20 degrees 30 degrees or whatever right so that is where you use regression one more example is stock price prediction so that is where again you will use regression so these are the various examples so you need to illustrate with examples and make sure you include those keywords like discrete and continuous so the next question is more about a little bit of a design related question to understand your concepts and things like that so it is how will you design a spam filter so how do you basically design on a spam filter so I think the main thing here is he's looking at probably understanding your concepts in what is the algorithm you will use or what is your understanding about difference between classification and regression and things like that and the process of course the methodology and the process so the best way to go about responding to this is we say that okay this is a classification problem because we want to find out whether an email is a spam or not spam so that we can apply the filter accordingly so first thing is to identify what type of problem it is so we have identified that it is a classification then the second step maybe to find out what kind of algorithm to use now since this is a binary classification problem logistic regression is a very common very common algorithm but however right as I said earlier also we can never say that okay for this particular problem this is exactly the algorithm that we can use so we can also probably try decision trees or even support vector machines for example SVM so we will kind of list down a few of these algorithms and we will say okay we want to we would like to try out these algorithms and then we go about taking your historical data which is the labeled data which are marked so you will have a bunch of emails and then you split that into training and test data sets you use your training data set to train your model that or your algorithm that you have used or rather the model actually so and you actually will have three models let's say you are trying to test out three algorithms so you will obviously have three models so you need to try all three models and test them out as well see which one gives the best accuracy and then you decide that you will go with that model okay so training and test will be done and then you zero in one one particular model and then you say okay this is the model we will use we will use and then go ahead and implement that or put that in production so that is the way you design a spam flip the next question is about random forest what is random forest so this is a very straightforward question however the response you need to be again a little careful while we are know what is random forest explaining this can sometimes be tricky so one thing is random forest is kind of in one way it is an extension of decision trees because it is basically nothing but you have multiple decision trees and trees will basically we will use for doing if it is classification mostly it is classification you will use the trees for classification and then you use voting for finding that the final class so that is the underlying but how will you explain this how will you respond to this so first thing obviously you will say that random forest is one of the algorithms and the more important thing that you need to probably the interviewer is waiting to hear is ensemble learner right so this is one type of ensemble learner what is ensemble learner ensemble learner is like a combination of algorithm so it is a learner which consists of more than one algorithm or more than one or maybe models okay so in case of random forest the algorithm is the same but instead of using one instance of it we use multiple instances of it and we use so in a way that is a random forest is an ensemble learner there are other types of ensemble learners where we have like we use different algorithms itself so you have one maybe logistic regression and a decision tree combined together and so on and so forth all there are other ways like for example splitting the data in a certain way and so on so that's all about ensemble we will not go into that but random forest itself I think the interview will be happy to hear this word ensemble learner and so then you go and explain how the random forest works so if the random forest is used for classification then we use what is known as a voting mechanism so basically how does it work let's say your random forest consists of 100 trees and each observation you pass through this forest and each observation let's say it is a classification problem binary classification 0 or 1 and you have 100 trees now if 90 trees say that it is a 0 and 10 of the trees say it is 1 you take the majority you may take a vote and since 90 of them are saying 0 you classify this as 0 then you take the next observation and so on so that is the way random forest works for classification if it is a regression problem it's somewhat similar but only thing is instead of vote what we will do is sorry integration remember what happens you actually calculate a value right so for example you're using regression to predict the temperature and you have 100 trees and each tree obviously will probably predict a different value of the temperature they may be close to each other but they may not be exactly the same value so these 100 trees so how do you now find the actual value the output for the entire forest right so you have outputs of individual trees which are a part of this forest but then you need to find the final output of the forest itself so how do you do that so in case of regression you take like an average or the mean of all the 100 trees right so this is also a way of reducing the error so maybe if you have only one tree and the plant tree makes a error it is basically 100% wrong or 100% right right but if you have on the other hand if you have a bunch of trees you are basically mitigating that reducing that okay so that is the way random forest works so the next question is considering the long list of machine learning algorithms how will you decide on which one to use so once again here there is no way to outright say that this is the algorithm that we will use for a given data set this is a very good question but then the response has to be like again there will not be one size fits all so we need to first of all you can probably shorten the list in terms of by saying okay whether it is a classification problem or it is a regression problem so that extent you can probably shorten the list because you don't have to use all of them if it is a classification problem you only can pick from the classification algorithms right so for example if it is a classification you cannot use linear regression algorithm or if it is a regression problem you cannot use SVM or maybe now you can use SVM but maybe a logistic regression right so to that extent you can probably shorten the list but still you will not be able to 100% decide on saying that this is the exact algorithm that I am going to use so the way to go about is you choose a few algorithms based on what the problem is you try out your data you train some models of these algorithms check which one gives you the lowest error or the highest accuracy and based on that you choose that particular algorithm okay all right then there can be questions around bias and variance so the question can be what is bias and variance in machine learning so you just need to give out a definition for each of these for example bias and machine learning it occurs when the predicted values are far away from the actual values so that is a bias okay and whereas they are all the values are probably they are far off but they are very near to each other though the predicted values are close to each other right while they are far off from the actual value but they are close to each other you see the difference so that is bias and then the other part is your variance now variance is when the predicted values are all over the place right so the variance is high that means it may be close to the target but it is kind of very scattered so the point the predicted values are not close to each other right in case of bias the predicted values are close to each other but they are not close to the target but here they may be close to the target but they may not be close to each other so they are a little bit more scattered so that is what in case of variance okay then the next question is about again related to bias and variance what is the trade off between bias and variance yes I think this is an interesting question because these two are heading in different directions so for example if you try to minimize the bias variance will keep going high and if you try to minimize the variance bias will keep going high and there is no way you can minimize both of them so you need to have a trade off saying that okay this is the level at which I will have my bias and this is the level at which I will have variance so the trade off is that pretty much that you decide what is the level you will tolerate for your bias and what is the level you will tolerate for variance and a combination of these two in such a way that your final results are not way off and having a trade off will ensure that the results are consistent right so that is basically the output is consistent and which means that they are close to each other and they are also accurate which that means they are as close to the target as possible right so if either of these is high then one of them will go off the track define precision and recall now again here I think it would be best to draw diagram and take the confusion matrix and it is very simple the definition is like a formula your precision is true positive by true positive plus false positive and your recall is true positive by true positive plus false negative okay so that's you can just show it in a mathematical way that's pretty much you know that can be shown that's the easiest way to define so the next question can be about decision tree what is decision tree pruning and why is it so basically decision trees are really simple to implement and understand but one of the drawbacks of decision trees is that it can become highly complicated as it grows right and the rules and the conditions can become very complicated and this can also lead to overfitting which is basically that during training you will get 100% accuracy but when you're doing testing you'll get a lot of errors so that is the reason pruning needs to be done so the purpose or the reason for doing decision tree pruning is to reduce overfitting or to cut down on overfitting and what is decision tree pruning it is basically that you reduce the number of branches because as you may be aware tree consists of the root node and then there are several internal nodes and then you have leaf nodes now if there are too many of these internal nodes that is when you face the problem of overfitting and pruning is the process of reducing those internal nodes all right so the next question can be what is logistic regression so basically logistic regression is one of the techniques used for performing classification especially binary classification now there is something special about logistic regression and there are a couple of things you need to be careful about first of all the name is a little confusing it is called logistic regression but it is used for classification so this can be sometimes confusing so you need to probably clarify that to the interviewers if it is required and they can also ask this like a trick question so that is one part second thing is the term logistic has nothing to do with the usual logistic that we talk about but it is derived from log so that the mathematical derivation was log and therefore the name logistic regression so what is logistic regression and how is it used so logistic regression is used for binary classification and output of a logistic regression is either a zero or a one and it varies so it's basically it calculates a probability between zero and one and we can set a threshold that can vary typically it is 0.5 so any value above 0.5 is considered as one and if the probability is below 0.5 it is considered as zero so that is the way we calculate the probability or the system calculates the probability and based on the threshold it sets a value of 0 or one which is like a binary classification 0 or 1 okay then we have a question around k nearest neighbor algorithm so explain k nearest neighbor algorithm so first of all what is a k nearest neighbor algorithm this is a classification algorithm so that is the first thing we need to mention and we also need to mention that the k is a number it is an integer and this is variable and we can define what the value of k should be it can be 2 3 5 7 and usually it is an odd number so that is something we need to mention technically it can be even number also but then typically it would be odd number and we will see why that is okay so based on that we need to classify objects okay we need to classify objects so again it will be very helpful to draw a diagram you know if you are explaining I think that will be the best way so draw some diagram like this and let's say we have three clusters or three classes existing and now you want to find for a new item that has come you want to find out which class this belongs right so you go about as the name suggests it you go about finding the nearest neighbors right the points which are closest to this and how many of them you will find that is what is defined by k now let's say our initial value of k was 5 so you will find the k the 5 nearest data points so in this case as it is illustrated these are the 5 nearest data points but then all 5 do not belong to the same class or cluster so there are one belonging to this cluster one the second one belonging to this cluster to three of them belonging to this third cluster okay so how do you decide that's exactly the reason we should as much as possible try to assign a odd number so that it becomes easier to assign this so in this case you see that the majority actually there are multiple classes then you go with the majority so since three of these items belong to this class we assign which is basically the in in this case the green or the tennis or the third cluster as I was talking about right so we assign it to this third class so in this case it is that's how it is decided okay so k nearest neighbors the first thing is to identify the number of neighbors that are mentioned as k so in this case it is k is equal to 5 so we find the five nearest points and then find out out of these five which class has the maximum number in that and and then the new data point is assigned to that class okay so that's pretty much how k nearest neighbors work all right so that brings us to the end of this module I hope you enjoyed this interview questions and hope you will be able to crack your next interview if any of these questions come up and in case you have other questions that you would like to be answered please feel free to put them down below this video in the comment section and we will try to maybe compile all of them and create a new video out of that okay with that I would like to sign off thank you very much have a good day hi there if you like this video subscribe to the simple learn youtube channel and click here to watch similar videos turn it up and get certified click here